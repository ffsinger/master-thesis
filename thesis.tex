\documentclass[a4paper,11pt,oneside]{report}
% Options: MScThesis, BScThesis, MScProject, BScProject, lablogo
\usepackage[MScThesis]{EPFLreport}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{calrsfs}

\title{Document representation for end to end encrypted search and machine learning}
\author{Florian Singer}
\epflsupervisor{Prof. Serge Vaudenay}
\companysupervisor{Nicolas Casademont}

%\dedication{...}
%\acknowledgments{...}

\begin{document}
\maketitle
%\makededication
%\makeacks

\begin{abstract}
The abstract serves as an executive summary of your project.
Your abstract should cover at least the following topics, 1-2 sentences for
each: what area you are in, the problem you focus on, why existing work is
insufficient, what the high-level intuition of your work is, maybe a neat
design or implementation decision, and key results of your evaluation.
\end{abstract}

\maketoc

% Memo: shortcuts VSCode
% Jump from code to pdf: Ctrl+Alt+J
% Jump from pdf to code: Ctrl+Click

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

% The introduction is a longer writeup that gently eases the reader into your thesis. 
% Use the first paragraph to discuss the setting.
% In the second paragraph you can introduce the main challenge that you see.
% The third paragraph lists why related work is insufficient.
% The fourth and fifth paragraphs discuss your approach and why it is needed.
% The sixth paragraph will introduce your thesis statement. Think how you can distill the essence of your thesis into a single sentence.
% The seventh paragraph will highlight some of your results
% The eights paragraph discusses your core contribution.

% This section is usually 3-5 pages.

The boom of artificial intelligence and machine learning seen in the past decade has had significant impact on societies, with the rise of recommender systems, image recognition software and large-scale advertisement platforms, to name a few. These advances can be beneficial, but they come with many risks and challenges that are often disregarded for the sake of short-term profit.

One of the main challenge is related to the sensitivity of the data used in these systems. As many different actors are usually participating in the training and usage of a machine learning model, the data providers need to place a high level of trust in the other actors.

This trust can be achieved through legal bindings, but only technological safeguards can provide real guarantees that the data will not be misused.

One of these safeguards is to hide the data with encryption. However, the mainstream cryptography of today only protects the data while in transit or at rest, and it cannot be used when performing operations on it. This limitation is fortunately lifted in a particular subset of cryptographic schemes, called homomorphic encryption schemes. These have the advantage of allowing some operations, primarily additions and multiplications, while keeping the numbers fully encrypted.

The downside of this approach is that it comes with severe performance and precision limitations, and is thus difficult to apply in the context of machine learning where computations are already gluttonous in resources. Nevertheless, this has been an active research field that has seen promising advances in the past decade [...].

The goal of this thesis is to identify, implement and evaluate the state-of-the-art solutions of homomorphically encrypted machine learning. It also aims to formalize and describe on a more granular level which steps of a machine learning algorithm can be protected using homomorphic encryption, and why.


%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
%%%%%%%%%%%%%%%%%%%%

% The background section introduces the necessary background to understand your work. 
% This is not necessarily related work but technologies and dependencies that must be resolved to understand your design and implementation.

% This section is usually 3-5 pages.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic regression}

Logistic regression is a simple and widely used model for classification. Given a set of $n$ training points $\mathbf{x}_i \in \mathbb{R}^d$ and the corresponding label $y_i \in \{-1,1\}$, the training phase consists of learning a vector of weights $\mathbf{w} \in \mathbb{R}^{d+1}$ minimizing the loss function:
\begin{equation}\label{logistic_reg_train}
    \mathcal{L}(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^{n} \log(1 + \exp(-\mathbf{z}_i^T \mathbf{w})) 
\end{equation}
where $\mathbf{z}_i = y_i \cdot (1, \mathbf{x}_i)$ for $i=1,...,n$.

A common method used to minimize the loss function is the gradient descent algorithm. It iteratively updates the weights vector by following the direction opposite to the gradient of the function:
\begin{align}\label{gradient_descent}
    \mathbf{w}^{t+1} & = \mathbf{w}^t - \gamma\nabla\mathcal{L}(\mathbf{w}) \nonumber \\ 
    & = \mathbf{w}^t + \frac{\gamma}{n} \sum_{i=1}^{n} \sigma(-\mathbf{z}_i^T \mathbf{w}^t) \cdot \mathbf{z}_i
\end{align}
where $\gamma$ is the learning rate and $\sigma$ is the sigmoid function (or logistic function) defined as:
\begin{equation}\label{sigmoid}
    \sigma(x) = \frac{1}{1 + e^{-x}}
\end{equation}

Classifying a new point $\mathbf{x}$ can be done as follows:
\begin{equation}\label{logistic_reg_pred}
    y = 
    \begin{cases}
        1, & \text{if } (1, \mathbf{x}) \cdot \mathbf{w} > 0 \\
        -1, & \text{otherwise} \\
    \end{cases}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural networks}

Neural networks are defined as a succession of \emph{layers}, each consisting of operations on a set of input neurons, which are typically real numbers.

Basic networks make use of \emph{fully connected} (or linear) layers, which are defined as such for an input vector $\mathbf{x} \in \mathbb{R}^d$, a weights matrix $W \in \mathbb{R}^{k \times d}$ and a bias vector $\mathbf{b} \in \mathbb{R}^k$:

\begin{equation}\label{linear_layer}
    h^{\mathrm{linear}}(\mathbf{x}) = W \mathbf{x} + \mathbf{b}
\end{equation}

Usually, a non-linear \emph{activation function} is applied to the output in order to learn more complex decision boundaries. The most common is $\mathrm{ReLU}(x) = \max(0, x)$, but the sigmoid (\ref{sigmoid}) or the hyperbolic tangent function are also often used.

The training phase is made of two main steps. First, a data point (or a batch of data points) is evaluated successively through the layers, in what we call the forward pass. Then, we perform a gradient descent by computing the gradients at each neuron and by updating the weights, starting from the end of the network. This is called the backward pass.

Once the model is trained, it can be used to classify new data points. For this operation, only the forward pass is required.

\subsection{Convolutional neural networks}

In many practical cases, particularly for image recognition, the input dimensions can be quite big. This is a problem, because a linear layer connects every input neuron with every output neuron. In order to improve performance and better exploit the spatiality properties of pixels in images, we can limit the connections of a pixel to its neighborhood. This amounts to applying a convolution.

For an image $x \in \mathbb{R}^{c \times h \times w}$ with $c$ channels, a height of $h$ and a width of $w$ pixels, a 2D convolutional layer is defined as follows for each output channel $i$:
\begin{equation}\label{conv2d_layer}
    h_i^{\mathrm{conv2d}}(x) = b_i + \sum_{k=1}^{c} w_{i, k} \star x_{k}
\end{equation}
where $b_i$ is the bias, $w_i \in \mathbb{R}^{c \times k_1 \times k_2}$ is the kernel and $\star$ is the 2D cross-correlation operation.

Another important type of layer is the \emph{pooling} layer, that will aggregate a neighborhood of neurons into one value. The type of aggregate is usually a \emph{max} operation or an \emph{average}.

The traditional architecture of a convolutional neural network starts with an alternate of convolutions, activation functions and pooling layers. Then, the input is flatten into one dimension and feeded to a sequence of fully connected layers.


%%%%%%%%%%%%%%%%%%%%%%%%
\section{Homomorphic encryption}

Schemes

Homomorphic operations


%%%%%%%%%%%%%%%%
\chapter{Design}
%%%%%%%%%%%%%%%%

Introduce and discuss the design decisions that you made during this project.
Highlight why individual decisions are important and/or necessary. Discuss
how the design fits together.

This section is usually 5-10 pages.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Threat model}

In order to justify the use of encrypted machine learning as a mitigation, it is important to identify and formalize the potential threats that an ML system can face. This also helps us to understand how this type of mitigation can be bypassed.

\subsection{Data-flow diagram}

\subsection{Adversaries}

\subsection{Threats}

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic regression}

Based on Kim et al. ~\cite{kim_logistic_2018}

Packing.

No bootstrapping, but it would be interesting to compare. The problem is that few libraries implement the bootstrapping operation.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural network}

2 packing alternatives with different trade-offs.



%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%

The implementation covers some of the implementation details of your project.
This is not intended to be a low level description of every line of code that
you wrote but covers the implementation aspects of the projects.

This section is usually 3-5 pages.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{FHE libraries}

\subsection{PALISADE}

Parameters (higher level)

Wrapper

Helper functions

\subsection{SEAL}

Parameters (lower level)

Wrapper

\subsection{Plaintext}

With numpy vectors, for debugging and performance comparisons.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic regression}

\subsection{Challenges}
Crypto and ML parameters.

Scale

Library-specific problems

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural network}



%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
%%%%%%%%%%%%%%%%%%%%

In the evaluation you convince the reader that your design works as intended.
Describe the evaluation setup, the designed experiments, and how the
experiments showcase the individual points you want to prove.

This section is usually 5-10 pages.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic regression}

Benchmarks of the different versions (plaintext, palisade, seal, sklearn) with the different metrics (accuracy, AUC, runtime, memory)

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural network}



%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

The related work section covers closely related work. Here you can highlight
the related work, how it solved the problem, and why it solved a different
problem. Do not play down the importance of related work, all of these
systems have been published and evaluated! Say what is different and how
you overcome some of the weaknesses of related work by discussing the 
trade-offs. Stay positive!

This section is usually 3-5 pages.


%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

In the conclusion you repeat the main result and finalize the discussion of
your project. Mention the core results and why as well as how your system
advances the status quo.

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

\end{document}
