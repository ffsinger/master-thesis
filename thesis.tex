\documentclass[a4paper,11pt,oneside]{report}
% Options: MScThesis, BScThesis, MScProject, BScProject, lablogo
\usepackage[MScThesis]{EPFLreport}

% Needs to be in this file if we want autocompletion to work
\addbibresource{thesis.bib}

\title{Secure and private machine learning using homomorphic encryption}
\author{Florian Singer}
\epflsupervisor{Prof. Serge Vaudenay}
\companysupervisor{Nicolas Casademont}

%\dedication{...}
\acknowledgments{I would like to thank the whole team at Ketl for their support during my internship, in particular Nicolas Casademont, James McGill and Samuel Halff for their valuable input on the written report and oral presentation, as well as Prof. Serge Vaudenay at EPFL for supervising this project.}

\begin{document}
\maketitle
%\makededication
%\makeacks

\begin{abstract}
% The abstract serves as an executive summary of your project.
% Your abstract should cover at least the following topics, 1-2 sentences for each: what area you are in, the problem you focus on, why existing work is insufficient, what the high-level intuition of your work is, maybe a neat design or implementation decision, and key results of your evaluation.

With the ever increasing use of machine learning models processing millions of data points, techniques for protecting this data have emerged.
The use of homomorphic encryption is promising but still suffers from strong performance issues and relatively low adoption outside of the academia.
We formalize the threats related to machine learning and attempt to solve the problems of secure model training and private inference using homomomorphic encryption.
Logistic regression models are used for training, and neural networks are used for inference.
We implement the solutions using two popular homomorphic encryption libraries and analyze the results in terms of runtime, memory usage and accuracy, with the perspective of a real-world deployement.
We find that heavy optimizations or security and privacy sacrifices need to be made for many use-cases, but we also explore trade-offs that can be acceptable, such as shifting some of the computations to the client.

\end{abstract}

\maketoc

% Memo: shortcuts VSCode
% Jump from code to pdf: Ctrl+Alt+J
% Jump from pdf to code: Ctrl+Click

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

% The introduction is a longer writeup that gently eases the reader into your thesis. 
% Use the first paragraph to discuss the setting.
% In the second paragraph you can introduce the main challenge that you see.
% The third paragraph lists why related work is insufficient.
% The fourth and fifth paragraphs discuss your approach and why it is needed.
% The sixth paragraph will introduce your thesis statement. Think how you can distill the essence of your thesis into a single sentence.
% The seventh paragraph will highlight some of your results
% The eights paragraph discusses your core contribution.

% This section is usually 3-5 pages.

The boom of artificial intelligence and machine learning seen in the past decade has had significant impact on societies, with the rise of recommender systems, image recognition software and large-scale advertisement platforms, to name a few. 
These advances can be beneficial, but they come with many risks and challenges that are often disregarded for the sake of short-term profit.

One of the main challenge is related to the sensitivity of the data used in these systems. 
As many different actors are usually participating in the training and usage of a machine learning (ML) model, the data providers need to place a high level of trust in the other actors.

This trust can be achieved through legal bindings, but only technological safeguards can provide real guarantees that the data will not be misused.

One of these safeguards is to hide the data with encryption. 
However, the mainstream cryptography of today only protects the data while in transit or at rest, and it cannot be used when performing operations on it. 
This limitation is fortunately lifted in a particular subset of cryptographic schemes, called homomorphic encryption (HE) schemes. 
These have the advantage of allowing some operations, primarily additions and multiplications, while keeping the numbers fully encrypted.

The downside of this approach is that it comes with severe performance and precision limitations, and is thus difficult to apply in the context of machine learning where computations are already gluttonous in resources. 
Nevertheless, this has been an active research field that has seen promising advances in the past decade.

The goal of this project is to identify, implement, evaluate and possibly improve the state-of-the-art solutions of homomorphically encrypted machine learning. It aims to determine if such solutions are practical for real-world systems, notably by exploring the trade-offs between security and performance.

In \autoref{chap:background}, we will introduce the basics of homomorphic encryption and two different types of ML models, namely logistic regression and neural networks. 
Then, we will formalize in \autoref{chap:design} the threats that exist in ML processes, and describe how they can be mitigated using homomorphic encryption.
More specifically, we will see how HE can protect data during both the training phase and the inference phase, and what it entails to transform an ML algorithm to a homomorphically-friendly version.
The implementation details and challenges are given in \autoref{chap:implementation}.
Finally, we present and analyze the different performance metrics in \autoref{chap:evaluation}.


%%%%%%%%%%%%%%%%%%%%
\chapter{Background}\label{chap:background}
%%%%%%%%%%%%%%%%%%%%

% The background section introduces the necessary background to understand your work. 
% This is not necessarily related work but technologies and dependencies that must be resolved to understand your design and implementation.

% This section is usually 3-5 pages.

This chapter introduces the main concepts that form the basis of this work, namely homomorphic encryption and machine learning.
The main ML models of interest for this thesis are logistic regression models and neural networks.
It will be useful for understanding the subsequent chapters.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic regression}

Logistic regression (LR) is a simple and widely used model for supervised classification. 
Given a set of $n$ training points $\mathbf{x}_i \in \mathbb{R}^d$ and the corresponding label $y_i \in \{-1,1\}$ among two classes, the training phase consists of learning a vector of weights $\mathbf{w} \in \mathbb{R}^{d+1}$ minimizing the loss function:
\begin{equation}\label{eq:logistic_reg_train}
    \mathcal{L}(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^{n} \log(1 + \exp(-\mathbf{z}_i^T \mathbf{w})) 
\end{equation}
where $\mathbf{z}_i = y_i \cdot (1, \mathbf{x}_i)$ for $i=1,...,n$.

A common method used to minimize the loss function is the gradient descent algorithm. 
It iteratively updates the weights vector by following the direction opposite to the gradient of the function:
\begin{align}\label{eq:gradient_descent}
    \mathbf{w}^{t+1} & = \mathbf{w}^t - \gamma\nabla\mathcal{L}(\mathbf{w}) \nonumber \\ 
    & = \mathbf{w}^t + \frac{\gamma}{n} \sum_{i=1}^{n} \sigma(-\mathbf{z}_i^T \mathbf{w}^t) \cdot \mathbf{z}_i
\end{align}
where $\gamma > 0$ is the learning rate and $\sigma$ is the sigmoid function (or logistic function) defined as:
\begin{equation}\label{eq:sigmoid}
    \sigma(x) = \frac{1}{1 + e^{-x}}
\end{equation}

Classifying a new data point $\mathbf{x}$ can be done as follows:
\begin{equation}\label{eq:logistic_reg_pred}
    y = 
    \begin{cases}
        1, & \text{if } (1, \mathbf{x}) \cdot \mathbf{w} > 0 \\
        -1, & \text{otherwise} \\
    \end{cases}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural networks}

Neural networks (NN) are defined as a succession of \emph{layers}, each consisting of operations on a set of input neurons, which are typically real numbers.

Basic networks make use of \emph{fully connected} (or \emph{linear}) layers, which are defined as such for an input vector $\mathbf{x} \in \mathbb{R}^d$, a weights matrix $W \in \mathbb{R}^{k \times d}$ and a bias vector $\mathbf{b} \in \mathbb{R}^k$:

\begin{equation}\label{eq:linear_layer}
    h^{\mathrm{linear}}(\mathbf{x}) = W \mathbf{x} + \mathbf{b}
\end{equation}

Usually, a non-linear \emph{activation function} is applied to the output in order to learn more complex decision boundaries. 
The most common is $\mathrm{ReLU}(x) = \max(0, x)$, but the sigmoid (\autoref{eq:sigmoid}) or the hyperbolic tangent function are also often used.

The training phase is made of two main steps. 
First, a data point (or a batch of data points) is evaluated successively through the layers, in what we call the forward pass. 
Then, we perform a gradient descent by computing the gradient of the loss function at each neuron and by updating the weights, starting from the end of the network. 
This is called the backward pass.

Once the model is trained, it can be used to classify new data points. 
For this operation, only the forward pass is required.
Each output neuron typically corresponds to a class, and the output values represent the likelihood that a point is part of this class.

\section{Convolutional neural networks}

In many practical cases, particularly for image recognition, the input dimensions can be quite big. 
This is a problem, because a linear layer connects every input neuron with every output neuron. 
In order to improve performance and better exploit the spatiality properties of pixels in images, we can limit the connections of a pixel to its neighborhood. 
This amounts to applying a convolution.

For an image $x \in \mathbb{R}^{c \times h \times w}$ with $c$ channels, a height of $h$ and a width of $w$ pixels, a 2D convolutional layer is defined as follows for each output channel $i$:
\begin{equation}\label{eq:conv2d_layer}
    h_i^{\mathrm{conv2d}}(x) = b_i + \sum_{k=1}^{c} w_{i, k} \star x_{k}
\end{equation}
where $b_i$ is the bias, $w_i \in \mathbb{R}^{c \times k_1 \times k_2}$ is the kernel and $\star$ is the 2D cross-correlation operation.

Another important type of layer is the \emph{pooling} layer, that will aggregate a neighborhood of neurons into one value. 
The type of aggregate is usually a \emph{max} operation or an \emph{average}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{figures/cnn.pdf}
  \caption{Traditional CNN architecture.}
  \label{fig:cnn}
\end{figure}

As illustrated in \autoref{fig:cnn}, the traditional architecture of a convolutional neural network (CNN) starts with an alternate of convolutions, activation functions and pooling layers. 
Each feature learned is encoded into a channel, and the convolutions will progressively increase the number of channels.
The role of the pooling layers is to reduce the size of these channels.
At some point, the input is flatten into one dimension and feeded to a sequence of fully connected layers.

This covers basic CNN architectures for supervised classification, but more advanced tools are often included in modern networks, such as \emph{batch normalization}, \emph{dropout}, or \emph{skip connections}.
There are also different types of architectures, such as \emph{recurrent neural networks} that are used for temporal data.
We will not go into such details, but support for these advanced methods could be done as future work.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Homomorphic encryption}

The basic idea of homomorphic encryption is to allow computations on encrypted data, which is not possible with most modern schemes used today. 
This problem has been studied for many years, with the first schemes supporting only one type of operation, such as the Paillier \cite{paillier_public-key_1999} cryptosystem supporting homomorphic additions. 

More formally, for a scheme supporting homomorphic additions on two elements $x$ and $y$ of a set, there should be an operation $\oplus$ as well as encryption and decryption operations $Enc$ and $Dec$ respectively, such that : 
\begin{equation}\label{eq:homomorphic_add}
  Dec(Enc(x) \oplus Enc(y)) = x + y
\end{equation}
The same principle holds for homomorphic multiplications, with a different operation $\otimes$.

The main revolution of the field was due to Gentry \cite{gentry_fully_2009} in 2009, when he presented a blueprint for constructing fully homomorphic encryption schemes. 
Fully homomorphic schemes support two types of primitive operations, typically additions and multiplications, and can evaluate arithmetic circuits of arbitrary depth thanks to a \emph{bootstrapping} procedure.

From this blueprint were born many cryptosystems that are used today, such as BGV \cite{brakerski_leveled_2012}, BFV \cite{fan_somewhat_2012}, or CKKS \cite{cheon_homomorphic_2017, cheon_full_2019}. 
The former two work with vectors of integers, and the latter deals with vectors of complex numbers.

Here is an overview of the basic operations available with CKKS:
\begin{itemize}
    \item \texttt{KeyGen}: Generate a public key and a private key.
    \item \texttt{Encode}: Encode a vector of complex numbers into a plaintext, which is a polynomial in the ring $R = \mathbb{Z}[X]/(X^N + 1)$.
    \item \texttt{Decode}: Decode a plaintext back into a vector.
    \item \texttt{Encrypt}: Encrypt a plaintext with the public key into an element of $R_q^2$ where $R_q = R/qR$ is a set of polynomials with coefficients modulo $q$.
    \item \texttt{Decrypt}: Decrypt a ciphertext with the private key.
    \item \texttt{Add}: Pairwise addition of two ciphertexts, or one ciphertext and one plaintext.
    \item \texttt{Multiply}: Pairwise multiplication of two ciphertexts/plaintext.
    \item \texttt{Rotate}: Permute the elements in a ciphertext by shifting them by $k$ positions.
    \item \texttt{Relinearize}: After a multiplication, the resulting ciphertext will be in $R_q^3$ and will continue to grow after each multiplication. 
    Relinearization is used to bring it back to $R_q^2$ and facilitate decryption.
    \item \texttt{Rescale}: Rounding operation performed to maintain the size of a ciphertext after a multiplication.
    \item \texttt{Bootstrap} \cite{cheon_bootstrapping_2018}: Refresh a ciphertext once it reached its maximum number of operations.
\end{itemize}

During the initialization phase, some parameters need to be set:

\begin{itemize}
  \item The \emph{ring degree} $N$ determines the size of the vectors that we encode. A higher degree also provides more security.
  \item The \emph{moduli chain} is a set of coprime integers of approximately the same size as $q$. 
  The number of elements should be at least the multiplicative depth of the function to be computed.
  \item The \emph{scale} of the plaintext, determining how many bits are consumed during a rescale operation.
\end{itemize}

These parameters need to be carefully tuned based on the problem at hand. 
Other parameters, such as the \emph{error distribution} or the \emph{key distribution}, are often set to the same standard value.

The basic concept of HE, while already powerful, is limited to two entities.
It is not possible to mix data encrypted with different keys.
In a scenario where multiple actors want to collaboratively use their data without revealing it to each other, it would be unsafe for all of them to use the same private key.
To solve this problem, two approaches have been proposed.

The first is \emph{Multi-Key Homomorphic Encryption} (MKHE).
It was first introduced by LÃ³pez-Alt et al. \cite{lopez-alt_--fly_2012} in 2012, as an extension of the NTRU HE scheme. 
Then, Chen et al. \cite{chen_efficient_2019} adapted this idea for bootstrappable schemes such as BFV and CKKS.
In this setting, any party can encrypt data and join the pool of computations.
There is no need for an initialization step between parties, which means that the participants do not need to be known in advance.
It is only during the decryption process that every party involved has to participate.

\emph{Multiparty Homomorphic Encryption} (MHE) is the second approach suggested by Mouchet al. \cite{mouchet_multiparty_2021}.
In this case, the key generation process must be done collaboratively with every participant in order to create a common public key.
This provides less flexibility, but the advantage lies in efficiency.
As opposed to MKHE, the size of ciphertexts, rotation keys, and relinearization keys do not grow with the number of parties.


%%%%%%%%%%%%%%%%
\chapter{Design}\label{chap:design}
%%%%%%%%%%%%%%%%

% Introduce and discuss the design decisions that you made during this project.
% Highlight why individual decisions are important and/or necessary. 
% Discuss how the design fits together.

% This section is usually 5-10 pages.

Now that the main mathematical and cryptographic concepts have been introduced, we focus on the problems to be solved.
In this chapter, we formalize the privacy and security threats that need to be mitigated.
We also describe our solutions to some of these issues from a high-level perspective, along with the important design decisions that were made.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Threat model}

Instead of focusing directly on specific issues, it is useful to draw an overview of the threats that a machine learning system can face.
This will help us to understand how homomorphic encryption can be used to mitigate some threats, and how these mitigations can be bypassed.

\subsection{Data-flow diagram}

\begin{figure}[p]
    \centering
    \includegraphics[width=\textwidth]{figures/dataflow-ml.pdf}
    \caption{Dataflow diagram for a general ML system. Rectangles represent entities, circles represent processes, arrows stand for data flows, and trust boundaries are illustrated with dashed lines. It is around these trust boundaries that we expect many threats to appear, because it often implies a communication between two entities.}
    \label{fig:dataflow}
\end{figure}

In order to better visualize how a machine learning system looks like, \autoref{fig:dataflow} attempts to model the data flow in a general case. 
This diagram could be refined for more concrete applications. 
For instance, the model provider and the data collector may be the same entity, which would remove a trust boundary. 
Another gray area is the feature extraction process that can happen in different ways: it could be done directly on the client and the data collector, or it could rely on an external service controlled by the model provider.

Thus, in this case, the goal of this diagram is to be a tool for identifying the threats, rather than an exhaustive reference.

\subsection{Threats}\label{sec:threats}

From surveys of the ML security literature \cite{liu_survey_2018, rigaki_survey_2021}, we can identify many types of threats:

\begin{itemize}
    \item Poisoning attacks
    \item Evasion/impersonation attacks
    \item Data ordering attacks
    \item Model extraction attacks
    \item Membership inference attacks
    \item Inversion attacks
    \item Attribute inference attacks (leakings features of samples)
    \item Property inference attacks (leaking features of datasets)
\end{itemize}

In many cases, the attacker needs to influence the training process by injecting or modifying the input data.

Let us take the example of a classification model based on textual data from documents. 
To carry out a poisoning attack, an adversary could create spam documents with uncommon words. 
This will reduce the utility of the system by replacing useful words in the feature vector. 
If the system lets adversaries define the labels, the model can also be poisoned with incorrect labels. 
This type of attack can be detected with the help of another model specialized in anomaly detection.

For evasion or impersonation attacks, the attack vector is the same but it involves carefully crafted inputs in order to produce a specific result. 
These attacks are often carried out to evade spam filters or to impersonate someone in a biometric recognition system.

In the same vein, data ordering attacks aim to reduce the utility or even bias the model by putting some data points first during training \cite{shumailov_manipulating_2021}. 
This could be used to introduce racist or sexist biases, for instance. 
Thus, it is important to shuffle the training data properly.

If the attacker has unlimited access to the prediction results, it may be possible to reconstruct and steal an approximation of the model (model extraction attack). 
The typical adversary carrying out this attack could be a competitor unwilling to dedicate the resources necessary for training a model from scratch.

In the case of membership inference attacks, inversion attacks, attribute and property inference attacks, the goal is to reconstruct part of the training dataset. 
This can either be general or target a single individual. To be successful, the attacker sometimes requires data poisoning \cite{hidano_model_2017}, or needs access to additional information, such as confidence levels or auxiliary datasets \cite{fredrikson_model_2015, wang_variational_2022}.

Referring back to the dataflow diagram in \autoref{fig:dataflow}, we see that most attacks occur on the trust boundary around the database, and the one around the client.
There is also a privacy risk when the training data is transmitted to the labeler, which could be a third party mandated by Amazon Mechanical Turk or other similar services.

As we will see in the following sections, there are two other threats that are of particular interest for us, one during the training process and one during the inference process.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Secure training}\label{sec:secure_training}

In addition to the threats that we identified in \autoref{sec:threats}, another one is that external actors, such as hackers or cloud providers, could observe the training process and exfiltrate the data. 
A potential defense mechanism is to train the model on homomorphically encrypted data.

In our case, we assume that the whole training data matrix is encrypted with the same key. 
The holder of this key is both the model owner and the data owner. 
The data should be encrypted in a secure enclave before sending it to the training service. 

If the data comes from external data owners and should not be visible to the model owner, we can design a protocol using multi-party homomorphic encryption. 
However, this feature is not available in many HE libraries.

If we distinguish the client (data and model owner) and the server (computing power provider), the steps of an encrypted training procedure can be described as follows:
% \begin{enumerate}
%     \item Client: initialize the keys and encrypt the data. Send the encrypted data and other objects or parameters required for homomorphic computations (e.g. rotation keys, relinearization keys).
%     \item Server: train the model. Send back the encrypted weights.
%     \item Client: decrypt the weights.
% \end{enumerate}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{ p{0.45\linewidth} | p{0.45\linewidth} }
    \hline
    Client & Server \\
    \hline
    1. Generate keys &  \\
    4. Send relinearization and rotation keys &  \\
    2. Encrypt training data &  \\
    3. Send encrypted data & \\
     & 5. Training \\
     & 6. Send back encrypted weights \\
    7. Decrypt weights &  \\
    \hline
    \end{tabular}
  \end{center}
\end{table}

As training can be a fairly complex and resource-intensive process, it is wise to start with a simple classification model. 
Logistic regression models are simple and have been extensively studied as part of the 2017 iDASH secure genome analysis competition. 
The winning solution has been proposed by Kim et al. ~\cite{kim_logistic_2018}. 
This will be the basis of our implementation.

However, it is necessary to make a few adjustments to remove some limitations. 
The input matrix is padded with zeros such that the number of columns is a power of two. 
This step is necessary for efficiently summing the elements of each row in logarithmic time. 
Then, the matrix is packed row by row into one or multiple ciphertexts. 
This method supports an arbitrary number of data points but requires that the number of features is lower than the number of slots, which is a power of two usually between $8`192$ and $32`768$. 

For the training step, a regularization term has also been added. 
Apart from its common usage in ML for avoiding overfitting, it has in this case the added advantage of improving the precision of the computations. 
This is because the learned coefficients are kept small, and the sigmoid approximation that we use is only precise for a certain interval around $0$.

The pseudocode for the encryption and training algorithm is given in the appendix (\autoref{algo:lr_training}).
It basically translates \autoref{eq:gradient_descent} into an algorithm containing only homomorphic operations on vectors.
We need to be careful to minimize the total number of operations, as well as the multiplicative depth.

The inference step described in \autoref{eq:logistic_reg_pred} can also be translated to a homomorphic version in order to have a completely encrypted pipeline, but this scenario will be described in the following section with a more complex model.


%%%%%%%%%%%%%%%%%%%%%%%%
\section{Private inference}\label{sec:private_inference}

\subsection{Overview}

In the case of an inference service provided by a model owner to a client, another threat is that the client's data can be abused. 
Even with a privacy policy, the client has no technical guarantee that this data will not be shared, stolen, or used for different purposes. 
As opposed to many attacks mentioned in \autoref{sec:threats}, this threat is independent of the ML model and involves only the data. 
Homomorphic encryption aims to reduce this risk, thus improving the overall trust in the system.

There are two approaches to this problem. 
In both cases, we assume that a model has been trained beforehand. 

In the most common approach, the client encrypts the data and sends it to the inference service, which learns nothing about the data and the inference result.

In the second approach, the client computes the inference by themselves with the trained model.
Unfortunately, few services will accept to send the model in cleartext, in fear of it being stolen.
However, the weights of the model can be encryted and sent to the client. 
This requires an additional round-trip of communications for the decryption step, which can reveal the inference result to the service if no precautions are made. 
In addition, the architecture of the model must be revealed to the client, which incurs the risk of it being stolen and retrained from scratch by a competitor. 
Due to these limitations, we will focus on the first approach, but the runtime estimates should not differ too much between the two, because the algorithms are quite similar.

This time, if we consider the client to be the data owner and the server to be the model owner, the steps are very similar to \autoref{sec:secure_training}:
% \begin{enumerate}
%     \item Client: initialize the keys and encrypt the data. Send the encrypted data and the required keys (relinearization or rotation keys).
%     \item Server: do the inference. Send back the result.
%     \item Client: decrypt the result.
% \end{enumerate}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{ p{0.45\linewidth} | p{0.45\linewidth} }
    \hline
    Client & Server \\
    \hline
    1. Generate keys &  \\
    4. Send relinearization and rotation keys &  \\
    2. Encrypt data &  \\
    3. Send encrypted data & \\
     & 5. Inference \\
     & 6. Send back encrypted label \\
    7. Decrypt label &  \\
    \hline
    \end{tabular}
  \end{center}
\end{table}

Note that the inference step can also be delegated to an external cloud provider without risks of compromising the data.
In addition, the first two steps only need to be done once if the client makes multiple inference requests over time.

Because predictions require much less resources than training, we can look at more complex models. 
As neural networks are extremely popular nowadays, a deep learning framework compatible with encrypted data would have many use-cases.

\subsection{Data packing}

This problem, especially for the special case of convolutional neural networks, has already been widely studied. One of the first solution was proposed by Dowlin et al. \cite{dowlin_cryptonets_2016} in 2016. 
One particularity of their approach lies in how they encode the data into vectors.
They pack the $n$ pixels of an input image into $n$ different ciphertexts. 
This is very efficient when batching multiple images (by putting the pixels of the first image in the first slots, the pixels of the second image in the second slots, etc) but it results in a lot of wasted space if only one image needs to be evaluated. 
From now on, we will call this method \emph{pixelwise packing}.

An alternative is to pack the pixels of an image one after the others in the same ciphertext. 
This results in a more efficient use of space when doing few inferences. For simplicity, we assume that all the pixels fit into a single ciphertext. 
We also separate each channel into its own ciphertext. 
For this reason, we call this method \emph{channelwise packing}.

Both methods have their own trade-offs that highly depend on the architecture of the network. 
They are evaluated side by side in \autoref{chap:evaluation}. 
From the theoretical complexity, we expect pixelwise packing to be best when kernels and fully connected layers are small.

% TODO? Theoretical complexity formula

In terms of memory, pixelwise packing may produce more ciphertexts, but it has the advantage of not needing any rotation key, which can take a few gigabytes when the ring dimension is large.

For our framework to be flexible on the architecture of the model, the same type of packing is kept throughout the layers. 
However, it would be possible to optimize a specific architecture by switching between multiple packings, as it is done for LoLa in \cite{brutzkus_low_2019}. 
The authors describe other packing methods optimized for certain tasks.

The pseudocode for a convolutional layer with channelwise packing is given as an example in \autoref{algo:conv2d_channelwise} of the appendix.

\subsection{Hybrid approach}\label{sec:hybrid_cnn}

As will be shown in \autoref{chap:evaluation}, a fully encrypted inference on a deep network can still be very time-consuming.
This is mainly due to the total depth of the network and to the first convolutional layers that extract features from an image.

These first layers are often quite generic and sometimes rely on publicly available pre-trained weights.
From the perspective of the model owner, making these layers public can be acceptable because most of the knowledge derived from the data is concentrated in the last classification layers.

Thus, an hybrid solution alternating between cleartext and encrypted parts could provide an important performance improvement at the expense of a reasonably small security sacrifice.
Although more computing power is required from the client, they keep the same guarantees in terms of data privacy.

Such an architecture is illustrated in \autoref{fig:hybrid_cnn}.
In this case, the client computes all the convolutions and pooling layers, then flattens and encrypts the result. 
The server only has to evaluate the fully connected layers on encrypted data.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{figures/layers.pdf}
  \caption{Hybrid CNN inference, the first part is done on the client in cleartext and the last layers are computed on the server.}
  \label{fig:hybrid_cnn}
\end{figure}

Another advantage of this approach is that the first layers are not restricted in the range of operations, because they are computed in cleartext. 
This means that no approximations are necessary for non-polynomial functions, and that popular layer types like ReLU or MaxPool are available.


%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}\label{chap:implementation}
%%%%%%%%%%%%%%%%%%%%%%%%

% The implementation covers some of the implementation details of your project.
% This is not intended to be a low level description of every line of code that you wrote but covers the implementation aspects of the projects.

% This section is usually 3-5 pages.

In the previous chapter, we described the different security threats that occur in ML systems. 
We identified that data can be at risk in both the training and the inference steps, and that homomorphic encryption is a way of protecting this data.
We started with a high-level description of the solutions, before focusing on how to apply these principles for specific ML models, namely logistic regression and neural networks.

This chapter covers the lower-level aspects of the solutions. 
It presents the libraries used and the architecture of the implementation.
It also highlights the challenges faced during development, as a way to introduce the limitations.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Backends}
\label{sec:backends}

There are multiple HE libraries providing the CKKS scheme. Most of them are implemented in \texttt{C++}, such as SEAL \cite{microsoft_corporation_microsoft_2022}, PALISADE \cite{palisade_palisade_2022} or HEAAN \cite{cryptolab_inc_heaan_2022}, but Lattigo \cite{tune_insight_sa_lattigo_2022} is implemented in Golang. 
A more complete survey and comparison of the different libraries can be found in \cite{viand_sok_2021}.

Each of these libraries has its own particularities, as there is not yet a complete standard for homomomorphic encryption (although the work has been started in \cite{albrecht_homomorphic_2021}). 
In the next subsections, we will introduce the ones that have been used in this project.

Despite these differences, we implemented a higher-level abstraction to unify the usage of these libraries, and to provide a more intuitive API for non-expert programmers. 
Thanks to this, the ML algorithms can be implemented independently of the backend. 
The set of available primitives is slightly reduced, because the relinearization and rescaling operations are performed automatically when necessary.
There are also utility functions to export the keys and ciphertexts into a \texttt{base64} encoded JSON object.
This gives an example of how clients and servers can share these objects in practice.

One of the requirements of the project was to code in Python. 
This means that our implementation needs bindings to use these \texttt{C++} libraries.

\subsection{PALISADE}

PALISADE is a widely used open-source library. 
It provides incomplete bindings for Python, but we can extend them as needed. 

In its basic usage, the library creates a context from a set of parameters and stores the keys needed for the evaluation (rotation keys, etc). 
It is slightly higher-level than other libraries by default, e.g., it rescales ciphertexts automatically. 
It also provides a wide set of utility functions, such as polynomial evaluation. 
Surprisingly, this function is not optimized to minimize the multiplicative depth, which is critical in our case.
Thus, we still need to implement our polynomial functions manually.

Multi-party HE is supported, but there is no bootstrapping procedure.

\subsection{SEAL}

SEAL, developed by Microsoft, is one of the most popular libraries. 
A complete set of Python bindings is already available \cite{hugang_seal-python_2022}.

It offers finer grained initialization parameters and a lower-level API (e.g., we need to set the moduli size at each level, and rescaling needs to be done manually).

It does not offer bootstrapping, nor a multi-party version.

\subsection{Plaintext}

When dealing with ciphertexts, it is extremely useful to have a cleartext pipeline that can be inspected for easier debugging. 
It can also be used to evaluate the cost of translating an algorithm to a homomorphic-encryption-friendly version, independently of the cost of encryption. 

For this backend, we used \texttt{numpy} vectors to represent the ciphertexts.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic regression}

The API for the logistic regression implementation is inspired by other ML libraries. 
It consists of a class providing a \texttt{train} and a \texttt{predict} function, along with their encrypted counterparts. 
Separate functions are available for encrypting and decrypting the data. 
This modularization is important for any real setting where the client and the server are different applications.

Complete pipelines are also provided as an example and for the evaluation.
It shows which parts should be run on the client, which parts should be run on the server, and which objects must be shared.

Since we do not have access to bootstrapping with SEAL and PALISADE, the number of gradient descent operations is limited. 
To overcome this, it would be possible to send the intermediate weights to the client and re-encrypt them. 
However, this way the client learns some information on the weights.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural networks}

Similarly to the logistic regression, a class wraps the encryption, prediction and decryption functions. 
It also translates layers defined with the \texttt{pytorch} library into an encrypted version.

The following layers are supported, for both the \emph{pixelwise} and the \emph{channelwise} packing methods:
\begin{itemize}
  \item Fully connected layer
  \item 2D convolution
  \item 2D average pooling
  \item Approximate sigmoid activation function (from \cite{kim_logistic_2018}) \\ $f(x) = 0.5 - 1.20096 \cdot (x/8) + 0.81562 \cdot (x/8)^3$ 
  \item Approximate $\tanh$ activation function (from \cite{podschwadt_classification_2020}) \\ $f(x) = -0.00163574303018748 \cdot x^3 + 0.249476365628036 \cdot x$ 
  \item Square activation function $f(x) = x^2$
\end{itemize}

The pipelines show an example of a client-server separation, as well as a non-encrypted training procedure for learning and storing the model weights.


%%%%%%%%%%%%%%%%%%%%%%%%
\section{Challenges}

Many problems arise when implementing or using these algorithms. 
First, the limited number of operations available with homomomorphic encryption forces us to find approximations or workarounds for some functions.
This is the case for the sigmoid or $\tanh$ functions.
These approximations are usually only precise in a certain range around zero. 
If the computations are not kept in this interval, they risk to overflow or become extremely biased.
Some other operations such as conditionals are also very expensive and complex to implement \cite{chialva_conditionals_2019}.

Then, we are in practice limited in the number of operations that we can chain. 
Even if CKKS with bootstrapping does not theoretically have this limitation, the imprecision of each computation, inherent to the scheme, will make the error grow continuously.
Also, the bootstrapping operation is not available in all libraries and still takes several seconds at best \cite{bossuat_efficient_2021}.

An important consideration is to be aware of the security level of the encryption. 
The security level is commonly defined as a number of bits. 
It estimates roughly the number of operations that an attacker would have to do on a ciphertext to learn some information about the private key or the data.
The Homomorphic Encryption Standard \cite{albrecht_homomorphic_2021} suggests values of the ring dimension $N$ and the coefficient modulus $q$ that achieve a security level of $128$, $192$, and $256$ bits. 
A security of $128$ bits is often the minimum for future-proof applications, but $80$ bits can also be considered sufficient given the computing power of current computers.
Thus, many researchers in the field of homomomorphic encryption use a level of $80$ bits to present their results, and we did the same in \autoref{chap:evaluation}.
It is however important to keep in mind that security can (and probably should) be increased for use in production, at the cost of an additional sacrifice in performance.

We can also encounter library-specific problems. 
For instance, the bit size of the moduli in the chain is not configurable in PALISADE. 
Because of this, the approximation error is sometimes too high at the last level, where the decryption is supposed to happen.
In addition, if the scale is not high enough at intermediate levels, the error can also become too big.

Another issue is that HE libraries are not yet interoperable, which means that clients and servers must use the same library when exchanging keys and ciphertexts. 
Hopefully, this will be resolved in the future with the current effort on standardization.

Finally, it is also more challenging to set the numerous ML parameters.
Without knowledge of the training data, a model owner has little means to evaluate a model and tune the parameters.
They cannot observe the evolution of the loss at the end of an iteration, which makes it impossible to apply a convergence criterion.
Solutions to these problems include having domain knowledge or using auxiliary datasets.


%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}\label{chap:evaluation}
%%%%%%%%%%%%%%%%%%%%

% In the evaluation you convince the reader that your design works as intended.
% Describe the evaluation setup, the designed experiments, and how the experiments showcase the individual points you want to prove.
% This section is usually 5-10 pages.

Now that the whole system has been presented in details in the two previous chapters, it is important to evaluate its performance and compare it to existing baselines.
It will serve to determine if such a system is usable in practice.
In this chapter, we provide benchmarks and analyses of the results for LR training and CNN inference with two different architectures.


%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logistic regression}\label{sec:eval_lr}

The following evaluation has been carried out on a small dataset of $80$ training samples and $20$ test samples. 
The samples have $2$ features and can be linearly separated in two classes.
We also compare these results with a bigger dataset of $16000$ training points and $4000$ test points.

For the cryptographic parameters, we take a ring dimension of $2^{14}$, a scale of $25$ and a security level of $80$ bits. 
The number of gradient descent iterations is $3$, the learning rate at iteration $t$ is the function $f(t) = 10 / (t+1)$, and the degree of the polynomial approximation of the sigmoid is $3$.
These tests were run on an \emph{AMD Ryzen 5 5600x} with $12$ threads and $32$ GB of RAM.

\begin{table}[h!]
  \begin{center}
    \caption{Total runtime of the LR pipelines}
    \label{table:lr_runtime}
    \vspace*{2mm}
    \begin{tabular}{ l r r r r r }
    \hline
     & plaintext & palisade & seal & standard & sklearn \\
    \hline
    Total runtime [s] - small & $0.00281$ & $1.57305$ & $9.34806$ & $0.00027$ & $0.00560$ \\
    Total runtime [s] - big & $0.01043$ & $6.21762$ & $19.03898$ & $0.00199$ & $0.00926$ \\
    % Keep this ?
    % Memory [MB] - small & $369$ & $1040$ & $3710$ & $367$ & $369$ \\
    % Memory [MB] - big & $369$ & $1060$ & $3820$ & $348$ & $349$ \\
    \hline
    \end{tabular}
  \end{center}
\end{table}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/lr_mem.pdf}
  \caption{Memory footprint of the LR pipelines}
  \label{fig:lr_memory}
\end{figure}

The total runtime and memory usage of the different pipelines are summarized in \autoref{table:lr_runtime} and \autoref{fig:lr_memory}. 
As a reminder, the \emph{plaintext} pipeline uses the same algorithm as the two encrypted pipelines (\emph{palisade} and \emph{seal}), but without encryption.
Two other pipelines serve as true baselines:
\begin{itemize}
  \item \emph{standard}: a simple implementation of a textbook logistic regression, without encryption.
  \item \emph{sklearn}: a commonly used ML library.
\end{itemize}

Note that the small training dataset can be put entirely in 1 ciphertext, while it takes 10 ciphertexts to encrypt the big dataset.

We first observe that the encrypted pipelines are orders of magnitude slower than the baselines.
By comparing the plaintext and the standard pipelines, it appears that transforming the algorithm to a homomorphically-friendly version has a negligible performance cost relatively to the overhead of encryption.
There is also not a big time difference between training on $80$ points and on $16000$ points, because part of the algorithm is independent on the number of input ciphertexts.

The model performance metrics (accuracy and area under the curve) are not different from the baseline (equal to 1 for the small dataset and close to 1 for the big one).

The approximate memory usage is almost identical to for the two datasets.
This indicates that it is dominated by the crypto context and not the ciphertexts.
In particular, the rotation key can take a few gigabytes if the ring dimension is high.
It also seems that SEAL consumes more memory than PALISADE, by a factor of at least $3$ in this case.

\autoref{table:lr_steps} breaks down the runtime at every step (encryption, training, decryption, etc) of the different backends for the small dataset.
PALISADE is faster than SEAL by around a factor $2$ for training and a factor $10$ for the initialization. 
These are the two most time-consuming steps in this setting.
Note that the encryption steps also include the packing of the data into vectors, which is why this step still takes a bit of time in the plaintext pipeline.

\begin{table}[h!]
  \begin{center}
    \caption{Runtime of the LR at each step, in seconds}
    \label{table:lr_steps}
    \vspace*{2mm}
    \pgfkeys{/pgf/number format/.cd, fixed, fixed zerofill, precision=5, 1000 sep={ }}
    \pgfplotstabletypeset[
      % multicolumn names,
      col sep=comma,
      display columns/0/.style={
        column name=$ $,
        column type={l},string type},
      display columns/1/.style={
        column name=Init,
        column type={r}},
      display columns/2/.style={
        column name=Encrypt train,
        column type={r}},
      display columns/3/.style={
        column name=Train,
        column type={r}},
      display columns/4/.style={
        column name=Encrypt test,
        column type={r}},
      display columns/5/.style={
        column name=Predict,
        column type={r}},
      display columns/6/.style={
        column name=Decrypt,
        column type={r}},
      every head row/.style={
          before row={\toprule},
          after row={
              % si{\second} & \si{\second} ... \\
              \midrule
          }
      },
      every last row/.style={after row=\bottomrule},
    ]{data/lr_small_steps.csv}
  \end{center}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural networks}\label{sec:eval_cnn}

\begin{table}[h!]
  \begin{center}
    \caption{Architecture of an homomorphically encrypted network as a proof-of-concept}
    \label{table:poc}
    \vspace*{2mm}
    \begin{tabular}{ c c c c c c c c }
    \hline
    Layer & In chan. & Out chan. & Kernel & In neurons & Out neurons & Padding & Stride \\
    \hline
    Conv2d & $1$ & $3$ & $5$ & $28 \times 28$ & $24 \times 24$ & $0$ & $0$ \\
    AvgPool2d & $3$ & $3$ & $3$ & $24 \times 24$ & $8 \times 8$ & $0$ & $3$ \\
    Square & $3$ & $3$ & & $8 \times 8$ & $8 \times 8$ &  &  \\
    Flatten \\
    Linear & & & & $192$ & $10$ & &  \\
    \hline
    \end{tabular}
  \end{center}
\end{table}

Let us now evaluate the performance of neural networks inference.
The network architecture used in this section (\autoref{table:poc}) is not designed for maximal accuracy, rather it shows the differences between different layer types.

\begin{table}[h!]
  \begin{center}
    \caption{Runtime of the CNN at each step, in seconds (pw = pixelwise and cw = channelwise)}
    \label{table:cnn_runtime}
    \vspace*{2mm}
    \pgfkeys{/pgf/number format/.cd, fixed, fixed zerofill, precision=5, 1000 sep={ }}
    \pgfplotstabletypeset[
      col sep=comma,
      display columns/0/.style={
        column name=$ $,
        column type={l},string type},
      display columns/1/.style={
        column name=Init client,
        column type={r}},
      display columns/2/.style={
        column name=Encrypt,
        column type={r}},
      display columns/3/.style={
        column name=Init server,
        column type={r}},
      display columns/4/.style={
        column name=Predict,
        column type={r}},
      display columns/5/.style={
        column name=Decrypt,
        column type={r}},
      display columns/6/.style={
        column name=Serialize,
        column type={r}},
      every head row/.style={
        before row={\toprule},
        after row={\midrule}
      },
      every last row/.style={after row=\bottomrule},
    ]{data/cnn_runtime.csv}
  \end{center}
\end{table}

The runtime of an encrypted inference with a convolutional neural network is presented in \autoref{table:cnn_runtime}. 
For this setting, we omit the PALISADE pipeline and show only the SEAL and plaintext pipelines with both packing methods, along with a baseline using \texttt{pytorch}.

The other difference with \autoref{sec:eval_lr} is that we simulate a true client-server communication and report the time needed to serialize and deserialize the request and the response.
These steps are summed in one column \emph{serialize} for readability.
As introduced in \autoref{sec:private_inference}, the client needs to send the encrypted data, the relinearization key, and the rotation keys if needed.
The server responds with the encrypted result.
For this evaluation, these objects are encoded in \texttt{base64} and sent in JSON via HTTP.

We observe that the serialization step takes a considerable amount of time because the data and the keys can be quite big. 
However, the runtime is still dominated by the prediction step, which takes around 2 minutes for this simple network.
The channelwise packing method was designed to reduce the latency for non-batched data, but it completely fails in this aspect by being even slower than pixelwise packing, except for the encryption step.
This result is due to the high number of rotation operations and the higher multiplicative depth.

\begin{table}[h!]
  \begin{center}
    \caption{Runtime of the CNN prediction at each layer, in seconds}
    \label{table:cnn_layers}
    \vspace*{2mm}
    \pgfkeys{/pgf/number format/.cd, fixed, fixed zerofill, precision=5, 1000 sep={ }}
    \pgfplotstabletypeset[
      col sep=comma,
      display columns/0/.style={
        column name=$ $,
        column type={l},string type},
      display columns/1/.style={
        column name=Convolution,
        column type={r}},
      display columns/2/.style={
        column name=Average pooling,
        column type={r}},
      display columns/3/.style={
        column name=Square,
        column type={r}},
      display columns/4/.style={
        column name=Linear,
        column type={r}},
      every head row/.style={
        before row={\toprule},
        after row={\midrule}
      },
      every last row/.style={after row=\bottomrule},
    ]{data/cnn_layers.csv}
  \end{center}
\end{table}

\autoref{table:cnn_layers} breaks down the prediction step to show the impact of each layer type. 
However, it is important to keep in mind that homomorphic computations are slower on the first levels, with fresh ciphertexts. 
Thus, a layer placed at the beginning of a network will be slower than its equivalent at the end of the network.
This is although not the case for the plaintext pipeline, which indicatest that the convolutional layer is still the slowest by far.
It also shows that channelwise packing has an advantage for linear layers and activation functions, compared to pixelwise packing.

% Keep the table ?
% \begin{table}[h!]
%   \begin{center}
%     \caption{Memory footprint of the CNN pipelines, in MB}
%     \label{table:cnn_memory}
%     \begin{tabular}{ c c c c c c }
%     \hline
%     & plaintext & seal & plaintext & seal & pytorch \\
%     & channelwise & channelwise & pixelwise & pixelwise & \\
%     \hline
%     Memory [MB] & $368$ & $1090$ & $402$ & $2620$ & $371$ \\
%     \hline
%     \end{tabular}
%   \end{center}
% \end{table}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/cnn_mem.pdf}
  \caption{Memory footprint of the CNN pipelines}
  \label{fig:cnn_memory}
\end{figure}

In terms of memory usage, shown in \autoref{fig:cnn_memory}, channelwise packing has a clear advantage.
It indicates that channelwise packing, even if it is slower, can be useful for systems with limited memory.
However, it is important to note that both the memory usage and the runtime will grow linearly with channelwise packing, while pixelwise packing will be able to batch multiple data points at no additional cost.


%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hybrid CNN}

In this section, we use a standard network architecture to see how our framework can be applied to a real use-case. 
These tests were run on an Azure \texttt{D16as\_v4} machine with $16$ threads and $64$ GB of RAM.

It should be accurate enough while still having reasonable performance.
Judging from the results of \autoref{sec:eval_cnn}, we need to choose a very simple architecture. 
The network is identical to LeNet-5 by LeCun \cite{lecun_gradient-based_1998}, except that we replace the $\tanh$ layers with a polynomial approximation of degree $3$. 
Layers are described in \autoref{table:lenet}.

\begin{table}[h!]
  \begin{center}
    \caption{Architecture of a homomorphically encrypted LeNet-5}
    \label{table:lenet}
    \begin{tabular}{ c c c c c c c c }
    \hline
    Layer & In chan. & Out chan. & Kernel & In neurons & Out neurons & Padding & Stride \\
    \hline
    Conv2d & $1$ & $6$ & $5$ & $28 \times 28$ & $28 \times 28$ & $2$ & $0$ \\
    TanhApprox & & & & & & &  \\
    AvgPool2d & $6$ & $6$ & $2$ & $28 \times 28$ & $14 \times 14$ & $0$ & $2$ \\
    Conv2d & $6$ & $16$ & $5$ & $14 \times 14$ & $10 \times 10$ & $0$ & $0$ \\
    TanhApprox & & & & & & &  \\
    AvgPool2d & $16$ & $16$ & $2$ & $10 \times 10$ & $5 \times 5$ & $0$ & $2$ \\
    Flatten \\
    Linear & & & & $400$ & $120$ & &  \\
    TanhApprox & & & & & & &  \\
    Linear & & & & $120$ & $84$ & &  \\
    TanhApprox & & & & & & &  \\
    Linear & & & & $84$ & $10$ & &  \\
    \hline
    \end{tabular}
  \end{center}
\end{table}

We also consider a hybrid version, as described in \autoref{sec:hybrid_cnn}, where only the last five layers after flattening are in the encrypted domain. In this case, the two first $\tanh$ layers do not need to be approximated and the learning rate is adapted accordingly.

The model is trained on the MNIST dataset, and the test samples are batched together in the same ciphertext with pixelwise packing. This lets us compute and compare the accuracy of the different versions in \autoref{table:lenet_results}, along with the runtime and memory usage.

The results indicate that even a simple network needs a lot of resources, with a latency of $176$ minutes and over $50$ GB of RAM used (in the case of SEAL).
However, if this latency can be tolerated, the time per inference is only of about $1.3$ seconds with a batch of $8192$ samples.

Furthermore, the hybrid version has a much smaller footprint and is also much faster, with an amortized time per inference of only $1$ millisecond. When it comes to accuracy, we notice a slight decrease due to the imprecisions in the encrypted computations.

% \begin{table}[h!]
%   \begin{center}
%     \caption{Results for LeNet-5}
%     \label{table:lenet_results}
%     \begin{tabular}{ c c c c c }
%     \hline
%     & pytorch & plaintext & seal & hybrid seal \\
%     & & pixelwise & pixelwise & pixelwise \\
%     \hline
%     Runtime [min] & $0.00468$ & $0.136$ & $176$ & $8.57$ \\
%     Memory [MB] & $392$ & $379$ & $51800$ & $2210$ \\
%     Accuracy & $0.976$ & $0.976$ & $0.953$ & $0.964$ \\
%     \hline
%     \end{tabular}
%   \end{center}
% \end{table}

\begin{table}[h!]
  \begin{center}
    \caption{Results for LeNet-5}
    \label{table:lenet_results}
    \vspace*{2mm}
    \begin{tabular}{ l r r r }
    \hline
    &  Runtime [min] &  Memory [MB] &  Accuracy \\
    \hline
    pytorch & $0.005$ & $392$ & $0.976$ \\ % &  &  &  \\
    plaintext-pixelwise & $0.136$ & $379$ & $0.976$ \\ % &  &  &  \\
    seal-pixelwise & $176.000$ & $51800$ & $0.953$ \\ % &  &  &  \\
    hybrid-seal-pixelwise & $8.570$ & $2210$ & $0.964$ \\
    \hline
    \end{tabular}
  \end{center}
\end{table}


%\subsection{PDF-split}
%The goal is to find a convolutional network that would be appropriate for the problem of finding logical separations between documents. 
%As this problem requires us to make a prediction for each page in a document, pixelwise packing is more appropriate for its ability to batch many inferences.


%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

% The related work section covers closely related work. 
% Here you can highlight the related work, how it solved the problem, and why it solved a different problem. Do not play down the importance of related work, all of these systems have been published and evaluated! Say what is different and how you overcome some of the weaknesses of related work by discussing the trade-offs. Stay positive!
% This section is usually 3-5 pages.

While some of the design and implementation decisions presented in the previous chapters are new, many other papers already suggest solutions for homomorphically encrypted machine learning.

One of the most influential is CryptoNets \cite{dowlin_cryptonets_2016}, which is a 5-layers CNN evaluated on the MNIST dataset. 
It inspired many other works that attempted to include optimizations to alleviate the strong performance limitations.

For instance, Badawi et al. \cite{badawi_privft_2020, badawi_towards_2020} implement an optimization using GPUs and design an algorithm for a specific network architecture (\emph{fasttext} for text classification). 
We started with this architecture before moving to a more generic framework.

Similarly to our hybrid model, LoLa \cite{brutzkus_low_2019} first computes a low dimension representation before encrypting it. 
It then selects an optimal packing method at each layer.

Lee et al. \cite{lee_privacy-preserving_2021} focus on a maximal accuracy by using a deep model (ResNet-20).

Compared to these solutions, our design is meant to be flexible to allow finding the desired trade-off between performance and utility.

Some works tackle NN training, but this requires either to use bootstrapping, or to delegate the weights update to the client \cite{mihara_neural_2020} or to a semi-honest server \cite{bellafqira_secure_2018}.

Surveys \cite{pulido-gaytan_privacy-preserving_2021,podschwadt_sok_2022} of privacy-preserving neural networks summarize these works, list the available libraries and explain the challenges.

Regarding logistic regression training, Kim et al. \cite{kim_logistic_2018} heavily inspired this work, with the exception of a few generalizations on the input size and the use of regularization.
The libraries used are also different, as they used HEAAN.

Other solutions include Bonte and Vercauteren \cite{bonte_privacy-preserving_2018} and Chen et al. \cite{chen_logistic_2018}.
Both are using the BFV encryption scheme.

Although NN and LR are the most studied models, there are also solutions for decision trees \cite{akavia_privacy-preserving_2019} and SVMs \cite{park_he-friendly_2020}.


%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

% In the conclusion you repeat the main result and finalize the discussion of your project. Mention the core results and why as well as how your system advances the status quo.

The field of homomorphic encryption is still emerging from the academia, and it still needs improvements before being usable in a real-world application. 
In particular, machine learning is a resource-intensive process that suffers greatly from the overhead of encryption.

Even simple models like logistic regression are several orders of magnitude slower to train, and require a lot of memory. 
The same applies for neural networks. Even by protecting only to the inference phase, it is still prohibitively expensive for many use-cases involving deep networks and big images.

However, the potential threats that would be solved by these solutions are sufficiently important to justify continuing the efforts in this field.
Optimizations can be found either at the cryptographic level or at the algorithmic level of ML models, for instance by using different packing methods or approximations.
Parallelization can be extremely valuable, and the desired security level also has a high impact on performance.

Another promising paradigm is to delegate some computations to the client and to keep the encrypted parts of the pipeline to a strict minimum while still providing enough security. 
The appropriate trade-off needs to be found on a case-by-case basis, depending on the capabilities of the client and those of the adversary.

It is also worth noting that future work could be done to extend our system to a multi-party setting by using MHE or MKHE. 
However, other approaches for multi-party training are also being researched. 
Instead of collecting all the data to a central server, \emph{Federated Learning} consists of training multiple local models by each data owner, and combining these models into one.
This way, the data never leaves its owner, even encrypted.
As only model weights are exchanged, this could remove the need of encryption entirely, but weights can still retain sensitive information.
Thus, some works such as \textsc{Poseidon} \cite{sav_poseidon_2021} combine this idea with MHE for better security guarantees.
Once again, the ideal approach highly depends on the use-case. 


\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\appendix
%%%%%%%%%%%%%%%%%%%%
\chapter{Algorithms}
%%%%%%%%%%%%%%%%%%%%

Here we describe the main algorithms used in this project. 
It is meant to illustrate the implementation of an algorithm with homomorphic encryption and CKKS, rather than being an exhaustive reference of all the algorithms implemented. 
Note that homomorphic operations ($\mathbf{Mult}$, $\mathbf{Add}$, $\mathbf{Rotate}$) are abstracted as mentioned in \autoref{sec:backends}, i.e. ciphertexts are automatically rescaled and relinearized after every multiplication when necessary.

In this type of algorithms, the computation of dot products is very common. 
It is done by first multiplying the two vectors, and successively adding the result with a rotated copy of the result. 
This outputs a vector containing the dot product at every element, but it can be masked if it needs to be only at one position.

By doing rotations of successive powers of two, the computational complexity of dot products can be reduced to $O(\log n)$ instead of $O(n)$. 
This is the reason why vectors are often padded with zeros to the next power of two.

%%%%%%%%%%%%%%%%%%%%
\section{Logistic regression training}

\autoref{algo:lr_training} describes how to train an encrypted LR model, from the encryption of the training data to the decryption of the trained weights.
It is inspired by \cite{kim_logistic_2018} but with less assumptions on the input size, and with the addition of a regularization term. 
For simplicity, we use traditional gradient descent instead of Nesterov's gradient update.

\begin{algorithm}

  \LinesNumbered
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwProg{Fn}{Function}{:}{}
  \SetKwInOut{Precondition}{Precondition}

  \KwIn{A matrix of samples $\mathbf{X} \in \mathbb{R}^{n\times (d-1)}$, the labels $\mathbf{y} \in \mathbb{R}^{n}$, and the ring degree $N$}
  \KwOut{The trained weights $\beta \in \mathbb{R}^{d}$}
  \Precondition{$d \leq N/2$}
  
  Let $s$ be the number of slots $N / 2$\;
  \CommentSty{// Encryption}\;
  Add a leading column of ones to $\mathbf{X}$\;
  Pairwise multiply the points of $\mathbf{X}$ with their corresponding label in $\mathbf{y}$, as a matrix $\mathbf{Z}$\;
  Pad the columns of $\mathbf{Z}$ with zeros to the next power of two $d_{pad}$\;
  Encrypt the matrix $\mathbf{Z}$ row by row into a set of ciphertexts $\{c_{z_i}\}_{i=1}^L$\;
  Let $r = 2^{\lceil\log_2\min(n, s / d_{pad})\rceil}$ be the number of rows in each ciphertext\;

  \CommentSty{// Training}\;
  Initialize a vector of size $s$ with random Xavier weights duplicated at each interval $d_{pad}$, and encrypt it into $c_\beta$\;
  Let $\mathbf{SigmoidApprox}$ be a sigmoid approximation function as defined in \cite{kim_logistic_2018}\;
  Let $x = [x_1, ..., x_s]$ be a mask vector keeping only the first element of each row, where $x_u = 1$ if $(u-1) \mod d_{pad}=0$, and $x_u=0$ otherwise\;
  Let $p$ a regularization vector with each non-padded element set to $-2/C$\;
  \For{$t=1,...,T$}{
    \For{$i=1,...,L$}{
      $c_i \leftarrow \mathbf{Mult}(c_{z_i}, c_\beta)$\;
      \For{$j=0,...,\log_2(d_{pad}) - 1$}{
        $c_i \leftarrow \mathbf{Add}(c_i, \mathbf{Rotate}(c_i, 2^j))$\;
      }
      $c_i \leftarrow \mathbf{Mult}(c_i, x)$\;
      \For{$j=0,...,\log_2(d_{pad}) - 1$}{
        $c_i \leftarrow \mathbf{Add}(c_i, \mathbf{Rotate}(c_i, -2^j))$\;
      }
      $c_i \leftarrow \mathbf{SigmoidApprox}(c_i)$\;
      $c_i \leftarrow \mathbf{Mult}(c_i, c_{z_i})$\;
    }
    $c \leftarrow \mathbf{Sum}(\{c_i\}_{i=1}^L)$\;
    \For{$j=\log_2(d_{pad}),...,\log_2(d_{pad}) + \log_2(r) - 1$}{
      $c \leftarrow \mathbf{Add}(c, \mathbf{Rotate}(c, 2^j))$\;
    }
    $c \leftarrow \mathbf{Add}(c, \mathbf{Mult}(c_\beta, p))$\;
    Let $l$ a learning rate vector with each non-padded element set to the learning rate at iteration $t$\;
    $c_\beta \leftarrow \mathbf{Add}(c_\beta, \mathbf{Mult}(c, l))$\;
  }

  \CommentSty{// Decryption}\;
  Decrypt $c_\beta$ into $\beta$ and truncate the vector to the first $d$ elements\;

\caption{Logistic regression training}
\label{algo:lr_training}

\end{algorithm}


%%%%%%%%%%%%%%%%%%%%
\section{2D convolution layer}

\autoref{algo:conv2d_channelwise} shows how to evaluate an encrypted 2D convolution layer that is part of a neural network. 
It uses \emph{channelwise} packing.

For more readability, this version does not support padding. The output feature maps are thus smaller than the input.

The version for \emph{pixelwise} packing is much simpler, as it only requires one homomorphic multiplication and addition in the inner loop. It is thus omitted here.

\begin{algorithm}

  \LinesNumbered
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwProg{Fn}{Function}{:}{}
  \SetKwInOut{Precondition}{Precondition}

  % A collection of ciphertexts $c$ where $c_{bk}$ is the ciphertext for sample $b=1,...,B$ and channel $k=1,...,K$
  \KwIn{A collection of ciphertexts for $B$ samples and $W_{in}$ input channels where each ciphertext represents a row-wise encoded image of size $R_{in} \times C_{in}$, a collection of kernels and biases, and the ring degree $N$}
  \KwOut{A collection $d$ of ciphertexts for $B$ samples and $W_{out}$ output channels}
  \Precondition{$R_{in} \cdot C_{in} \leq N/2$}

  \For{$b=1,...,B$}{
    \For{$m=1,...,W_{out}$}{
      \For{$l=1,...,W_{in}$}{
        Let $k$ be the kernel for input channel $w_{in}$ and output channel $w_{out}$, padded with zeros to a shape of $R_{in} \times C_{in}$ and row-wise encoded\;
        Let $x$ be the ciphertext for sample $b$ and input channel $w_{in}$\;
        \For{$i=1,...,R_{out}$}{
          \For{$j=1,...,C_{out}$}{
            Let $k_{rot}$ be the kernel $k$ rotated by $i \cdot C_{in} + j$\ positions to the right\;
            $z \leftarrow \mathbf{Mult}(x, k_{rot})$\;
            \For{$q=0,...,\log_2(N / 2) - 1$}{
              $z \leftarrow \mathbf{Add}(z, \mathbf{Rotate}(z, -2^q))$\;
            }
            Let $a$ be a zero vector with element $i \cdot C_{out} + j$\ set to $1$\;
            $z \leftarrow \mathbf{Mult}(z, a)$\;
            $d_{bm} \leftarrow \mathbf{Add}(d_{bm}, z)$\;
          }
        }
      }
      Let $y$ be a vector filled with the bias value for channel $m$\;
      $d_{bm} \leftarrow \mathbf{Add}(d_{bm}, y)$\;
    }
  }
  

\caption{2D convolution layer (channelwise packing)}
\label{algo:conv2d_channelwise}

\end{algorithm}


% \chapter{Other appendix}

\end{document}
